{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "######################################################################################\n",
    "#####################################################################################\n",
    "######################################################################################\n",
    "############################Code by PlanteafX#########################################\n",
    "##########################Haseeb Ahmad (18i-1579)####################################\n",
    "#######################Muhammad Zargham Masood (18i-0464)############################\n",
    "###########################Masood Ahmad (18i-0755)##################################\n",
    "####################################################################################\n",
    "####################################################################################\n",
    "\n",
    "\n",
    "\n",
    "#Import libraries of tensorflow and other\n",
    "import numpy as np\n",
    "from re import sub\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import metrics\n",
    "\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation, Flatten, Dropout, Dense\n",
    "\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "\n",
    "print(\"Done importing packages!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE=(132, 132)     #Image size defines the input size i.e., size of image being given to model\n",
    "BATCH_SIZE=16         #Defines the Batch Size \n",
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1.0/255.0) #Imagedatagenerator for training set\n",
    "val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1.0/255.0)  #Imagedatagenerator for validation set\n",
    "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1.0/255.0)  #Imagedatagenerator for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pick the training set from the directory using imagedatagenerator train_datagen\n",
    "#train_set in our case is 90%\n",
    "train_set = train_datagen.flow_from_directory(\n",
    "    './new/train', \n",
    "    shuffle=False, \n",
    "    seed=42,\n",
    "    color_mode=\"rgb\", \n",
    "    class_mode=\"categorical\",\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pick the validation set from the directory using imagedatagenerator val_datagen\n",
    "#val_set in our case is 7%\n",
    "val_set = val_datagen.flow_from_directory(\n",
    "    './new/val', \n",
    "    shuffle=False, \n",
    "    seed=42,\n",
    "    color_mode=\"rgb\", \n",
    "    class_mode=\"categorical\",\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pick the test set from the directory using imagedatagenerator test_datagen\n",
    "#test_set in our case is 3%\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "    './new/test', \n",
    "    shuffle=False, \n",
    "    seed=42,\n",
    "    color_mode=\"rgb\", #color mode is kept rgb beacuse of the colored images\n",
    "    class_mode=\"categorical\",  #as data is in classes so it is important to pick in categorical form\n",
    "    target_size=IMG_SIZE,  #defines the image size (132x132)\n",
    "    batch_size=BATCH_SIZE) #defines the batch size (16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Check the indices of test set whether they are correct or not\n",
    "test_set.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#downloading the model from keras with imagenet weights\n",
    "base_model = tf.keras.applications.EfficientNetB5(\n",
    "                                include_top=False,\n",
    "                                weights='imagenet',\n",
    "                                input_shape=IMG_SIZE+(3,),\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.summary() #prints the base_model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_batch, label_batch = next(iter(train_set)) #get the image batch and label batch of each batch present in the train set\n",
    "feature_batch = base_model(image_batch) #determining the shape of output by pretained model\n",
    "print(feature_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D() #make pooling layer\n",
    "feature_batch_average = global_average_layer(feature_batch)  #determining the average shape of feature batch\n",
    "\n",
    "print(feature_batch_average.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_layer = tf.keras.layers.Dense(train_set.num_classes, activation=\"softmax\") #make the dense layer for prediction\n",
    "prediction_batch = prediction_layer(feature_batch_average) \n",
    "print(prediction_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(132, 132, 3)) #making the input layer\n",
    "x = base_model(inputs, training=True)#adding the input layer in front of pretained model\n",
    "x = global_average_layer(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x) #adding the dropout layer\n",
    "outputs = prediction_layer(x) #adding the prediction layer\n",
    "model = tf.keras.Model(inputs, outputs)# making the model by giving input layer and output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary() #prints the summary of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metrics that are shown  and stroed during training time\n",
    "METRICS = [\n",
    "      metrics.CategoricalAccuracy(name='accuracy'),\n",
    "      metrics.Precision(name='precision'),\n",
    "      metrics.Recall(name='recall'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#early stopping technique to stop the training when there is no chnage of loss in five epochs\n",
    "callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"loss\",\n",
    "    patience=5,\n",
    "    mode=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_learning_rate = 0.001 #initial learning rate \n",
    "\n",
    "#uses adam as a optimizer with beta1 = 0.9 and beta2 = 0.999. Uses categorical cross entropy loss because the data is multiclass\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate, beta_1=0.9,\n",
    "    beta_2=0.999),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
    "              metrics=METRICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_epochs = 10   #initializes initial epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the model using val_set and train_set (It took almost 6 hrs to complete training on 10 epochs)\n",
    "\n",
    "history = model.fit(train_set,\n",
    "                    epochs=initial_epochs,validation_data=val_set, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#to evaluate the test_set on our trained model\n",
    "model.evaluate(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plots the graph of training accuracy and validation accuracy with respect to epochs\n",
    " \n",
    "plt.plot(history.history['accuracy']) #training accuracy from history\n",
    "plt.plot(history.history['val_accuracy']) #validation accuracy from history\n",
    "plt.title('model accuracy') #title of graph\n",
    "plt.ylabel('accuracy') #y-label\n",
    "plt.xlabel('epoch') #x-label\n",
    "plt.legend(['train', 'val'], loc='upper left') # defining legends\n",
    "plt.show() #plotting\n",
    "\n",
    "\n",
    "#Plots the graph of training loss and validation loss with respect to epochs\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#make prediction on test set\n",
    "pred=model.predict(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save weights in .h5 format for later use\n",
    "model.save_weights(\"./results/ResNetTest_model_weigts.h5\")\n",
    "print(\"\\nModel weights saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#make an array of 56x56 for confusion matrix because our total species are 65\n",
    "a = np.zeros((56, 56))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred_labels=[] #array for every prediction of image in dataset\n",
    "for k in range(len(pred)):\n",
    "    y_pred_labels.append(np.argmax(pred[k])) #extracts the max from the prediction(predicted class) and stores it in array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "number_of_examples = len(test_set.filenames) #extract filenames from test set\n",
    "number_of_generator_calls = math.ceil(number_of_examples / (1.0 * BATCH_SIZE)) \n",
    "# 1.0 above is to skip integer division\n",
    "\n",
    "test_labels = [] # true labels array of test set\n",
    "\n",
    "for i in range(0,int(number_of_generator_calls)):\n",
    "    test_labels.extend(np.array(test_set[i][1])) #extracts the lavel array from test set of every image and stores it in an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_labels=[] #true labels\n",
    "for k in range(len(test_labels)):\n",
    "    y_test_labels.append(np.argmax(test_labels[k])) #get the max of true labels array for every image and stores it in an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(len(y_pred_labels)):\n",
    "    #if (y_test_labels[k]== y_pred_labels[k]):\n",
    "    a[y_test_labels[k]][y_pred_labels[k]] += 1  #appending the true labels that model predicted it right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names= ['Alstonia_Scholaris___healthy',\n",
    " 'Apple___Apple_scab',\n",
    " 'Apple___Black_rot',\n",
    " 'Apple___Cedar_apple_rust',\n",
    " 'Apple___healthy',\n",
    " 'Arjun___healthy',\n",
    " 'Basil___healthy',\n",
    " 'Blueberry___healthy',\n",
    " 'Cherry_(including_sour)___Powdery_mildew',\n",
    " 'Cherry_(including_sour)___healthy',\n",
    " 'Chinar___healthy',\n",
    " 'Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot',\n",
    " 'Corn_(maize)___Common_rust_',\n",
    " 'Corn_(maize)___Northern_Leaf_Blight',\n",
    " 'Corn_(maize)___healthy',\n",
    " 'Gauva___healthy',\n",
    " 'Grape___Black_rot',\n",
    " 'Grape___Esca_(Black_Measles)',\n",
    " 'Grape___Leaf_blight_(Isariopsis_Leaf_Spot)',\n",
    " 'Grape___healthy',\n",
    " 'Jamun___healthy',\n",
    " 'Jatropha___healthy',\n",
    " 'Lemon___Citrus_Leaf_Miner',\n",
    " 'Lemon___healthy',\n",
    " 'Mango___healthy',\n",
    " 'Orange___Haunglongbing_(Citrus_greening)',\n",
    " 'Peach___Bacterial_spot',\n",
    " 'Peach___healthy',\n",
    " 'Pepper,_bell___Bacterial_spot',\n",
    " 'Pepper,_bell___healthy',\n",
    " 'Pomegranate___healthy',\n",
    " 'Pongami_Pinnata___healthy',\n",
    " 'Raspberry___healthy',\n",
    " 'Rice___BrownSpot',\n",
    " 'Rice___Healthy',\n",
    " 'Rice___Hispa',\n",
    " 'Rice___LeafBlast',\n",
    " 'Soybean___healthy',\n",
    " 'Squash___Powdery_mildew',\n",
    " 'Strawberry___Leaf_scorch',\n",
    " 'Strawberry___healthy',\n",
    " 'Tomato___Bacterial_spot',\n",
    " 'Tomato___Early_blight',\n",
    " 'Tomato___Late_blight',\n",
    " 'Tomato___Leaf_Mold',\n",
    " 'Tomato___Septoria_leaf_spot',\n",
    " 'Tomato___Spider_mites Two-spotted_spider_mite',\n",
    " 'Tomato___Target_Spot',\n",
    " 'Tomato___Tomato_Yellow_Leaf_Curl_Virus',\n",
    " 'Tomato___Tomato_mosaic_virus',\n",
    " 'Tomato___healthy',\n",
    " 'wheat___Healthy',\n",
    " 'wheat___stripe_rust']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#ploting the confusing matrix\n",
    "df_cm = pd.DataFrame(a, index = [i for i in class_names],\n",
    "                  columns = [i for i in class_names])\n",
    "plt.figure(figsize = (50,20)) #showing the confusing matrix\n",
    "sn.heatmap(df_cm, annot=True, fmt = 'g') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
